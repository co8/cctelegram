/**
 * Task 21.6: Tier 3 File Watcher System Implementation
 * File-based response queue with JSON serialization, atomic operations, and debounced processing
 */

use std::collections::HashMap;
use std::path::{Path, PathBuf};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{Mutex, RwLock};
use tokio::time::sleep;
use serde::{Deserialize, Serialize};
use chrono::{DateTime, Utc};
use uuid::Uuid;
use tracing::{info, warn, error, debug};
use anyhow::{Result, Context};
use tempfile::NamedTempFile;
use std::fs;
use notify::{Watcher, RecursiveMode, Event as NotifyEvent, EventKind};

use crate::internal_processor::{ResponsePayload, ProcessingResult};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FileQueueEntry {
    pub id: String,
    pub correlation_id: String,
    pub payload: ResponsePayload,
    pub created_at: DateTime<Utc>,
    pub processed_at: Option<DateTime<Utc>>,
    pub retry_count: u32,
    pub status: FileQueueStatus,
    pub error: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum FileQueueStatus {
    #[serde(rename = "pending")]
    Pending,
    #[serde(rename = "processing")]
    Processing,
    #[serde(rename = "completed")]
    Completed,
    #[serde(rename = "failed")]
    Failed,
    #[serde(rename = "retry")]
    Retry,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FileWatcherMetrics {
    pub total_processed: u64,
    pub successful_processed: u64,
    pub failed_processed: u64,
    pub average_processing_time_ms: u64,
    pub queue_depth: usize,
    pub recovery_operations: u64,
    pub debounce_events: u64,
}

impl Default for FileWatcherMetrics {
    fn default() -> Self {
        Self {
            total_processed: 0,
            successful_processed: 0,
            failed_processed: 0,
            average_processing_time_ms: 0,
            queue_depth: 0,
            recovery_operations: 0,
            debounce_events: 0,
        }
    }
}

#[derive(Debug)]
struct DebouncedEvent {
    path: PathBuf,
    last_modified: Instant,
}

#[derive(Debug)]
pub struct FileTierProcessor {
    queue_directory: PathBuf,
    processed_directory: PathBuf,
    failed_directory: PathBuf,
    debounce_duration: Duration,
    max_retries: u32,
    metrics: Arc<RwLock<FileWatcherMetrics>>,
    debounced_events: Arc<Mutex<HashMap<PathBuf, DebouncedEvent>>>,
    _watcher: notify::RecommendedWatcher,
    processing_queue: Arc<Mutex<Vec<FileQueueEntry>>>,
}

impl FileTierProcessor {
    const DEBOUNCE_WINDOW_MS: u64 = 500;
    const MAX_RETRY_COUNT: u32 = 3;
    const QUEUE_FILE_EXTENSION: &'static str = "json";
    
    pub fn new<P: AsRef<Path>>(queue_base_path: P) -> Result<Self> {
        let base_path = queue_base_path.as_ref();
        
        // Create directory structure
        let queue_directory = base_path.join("queue");
        let processed_directory = base_path.join("processed"); 
        let failed_directory = base_path.join("failed");
        
        for dir in [&queue_directory, &processed_directory, &failed_directory] {
            fs::create_dir_all(dir)
                .with_context(|| format!("Failed to create directory: {}", dir.display()))?;
        }
        
        let debounced_events = Arc::new(Mutex::new(HashMap::new()));
        let processing_queue = Arc::new(Mutex::new(Vec::new()));
        
        // Set up file watcher
        let debounced_events_clone = Arc::clone(&debounced_events);
        let queue_directory_clone = queue_directory.clone();
        
        let watcher = notify::recommended_watcher(move |res: notify::Result<NotifyEvent>| {
            match res {
                Ok(event) => {
                    if let Err(e) = Self::handle_file_event(
                        &event, 
                        &queue_directory_clone,
                        Arc::clone(&debounced_events_clone)
                    ) {
                        error!("Error handling file event: {}", e);
                    }
                }
                Err(e) => error!("File watcher error: {:?}", e),
            }
        })?;
        
        let mut watcher = watcher;
        watcher.watch(&queue_directory, RecursiveMode::NonRecursive)
            .with_context(|| format!("Failed to watch directory: {}", queue_directory.display()))?;
            
        info!("üîß [TIER-3-FILE] File watcher processor initialized");
        info!("üìÅ Queue directory: {}", queue_directory.display());
        info!("‚úÖ Processed directory: {}", processed_directory.display());
        info!("‚ùå Failed directory: {}", failed_directory.display());
        
        let processor = Self {
            queue_directory,
            processed_directory,
            failed_directory,
            debounce_duration: Duration::from_millis(Self::DEBOUNCE_WINDOW_MS),
            max_retries: Self::MAX_RETRY_COUNT,
            metrics: Arc::new(RwLock::new(FileWatcherMetrics::default())),
            debounced_events,
            _watcher: watcher,
            processing_queue,
        };
        
        // Perform recovery on startup
        if let Err(e) = processor.recover_pending_entries() {
            warn!("Failed to recover pending entries: {}", e);
        }
        
        Ok(processor)
    }
    
    fn handle_file_event(
        event: &NotifyEvent,
        queue_directory: &Path,
        debounced_events: Arc<Mutex<HashMap<PathBuf, DebouncedEvent>>>
    ) -> Result<()> {
        match &event.kind {
            EventKind::Create(_) | EventKind::Modify(_) => {
                let queue_dir = queue_directory.to_path_buf();
                let relevant_paths: Vec<PathBuf> = event.paths.iter()
                    .filter(|path| {
                        path.extension().map_or(false, |ext| ext == Self::QUEUE_FILE_EXTENSION) 
                        && path.starts_with(&queue_dir)
                    })
                    .cloned()
                    .collect();
                
                if !relevant_paths.is_empty() {
                    // Add to debounced events
                    tokio::spawn(async move {
                        let mut events = debounced_events.lock().await;
                        for path in relevant_paths {
                            events.insert(path.clone(), DebouncedEvent {
                                path: path.clone(),
                                last_modified: Instant::now(),
                            });
                            debug!("üìù [TIER-3-FILE] Debounced file event: {}", path.display());
                        }
                    });
                }
            }
            _ => {} // Ignore other event types
        }
        Ok(())
    }
    
    pub async fn queue_response(&self, payload: ResponsePayload, correlation_id: &str) -> Result<String> {
        let entry_id = Uuid::new_v4().to_string();
        
        let queue_entry = FileQueueEntry {
            id: entry_id.clone(),
            correlation_id: correlation_id.to_string(),
            payload,
            created_at: Utc::now(),
            processed_at: None,
            retry_count: 0,
            status: FileQueueStatus::Pending,
            error: None,
        };
        
        // Write to queue using atomic operation
        self.write_queue_entry_atomic(&queue_entry).await
            .with_context(|| format!("Failed to queue entry {}", entry_id))?;
        
        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.queue_depth += 1;
        }
        
        info!("üìù [TIER-3-FILE] Queued response (ID: {}, correlation: {})", entry_id, correlation_id);
        
        Ok(entry_id)
    }
    
    async fn write_queue_entry_atomic(&self, entry: &FileQueueEntry) -> Result<()> {
        let filename = format!("{}.{}", entry.id, Self::QUEUE_FILE_EXTENSION);
        let target_path = self.queue_directory.join(&filename);
        
        // Create temporary file in the same directory to ensure atomic move
        let temp_file = NamedTempFile::new_in(&self.queue_directory)
            .with_context(|| "Failed to create temporary file")?;
            
        // Write JSON to temporary file
        let json_data = serde_json::to_string_pretty(entry)
            .with_context(|| "Failed to serialize queue entry")?;
            
        tokio::fs::write(temp_file.path(), json_data).await
            .with_context(|| "Failed to write to temporary file")?;
            
        // Atomically move temporary file to final location
        temp_file.persist(&target_path)
            .with_context(|| format!("Failed to persist file to {}", target_path.display()))?;
            
        debug!("üíæ [TIER-3-FILE] Atomic write completed: {}", filename);
        
        Ok(())
    }
    
    pub async fn process_debounced_events(&self) -> Result<Vec<ProcessingResult>> {
        let mut results = Vec::new();
        let mut events_to_process = Vec::new();
        
        // Collect debounced events that are ready for processing
        {
            let mut debounced = self.debounced_events.lock().await;
            let now = Instant::now();
            
            debounced.retain(|_path, event| {
                if now.duration_since(event.last_modified) >= self.debounce_duration {
                    events_to_process.push(event.path.clone());
                    false // Remove from debounced events
                } else {
                    true // Keep for future processing
                }
            });
        }
        
        if !events_to_process.is_empty() {
            let mut metrics = self.metrics.write().await;
            metrics.debounce_events += events_to_process.len() as u64;
        }
        
        // Process each debounced file
        for file_path in events_to_process {
            match self.process_queue_file(&file_path).await {
                Ok(result) => {
                    results.push(result);
                    info!("‚úÖ [TIER-3-FILE] Processed debounced file: {}", file_path.display());
                }
                Err(e) => {
                    error!("‚ùå [TIER-3-FILE] Failed to process file {}: {}", file_path.display(), e);
                }
            }
        }
        
        Ok(results)
    }
    
    async fn process_queue_file(&self, file_path: &Path) -> Result<ProcessingResult> {
        let start_time = Instant::now();
        
        // Read and parse queue entry
        let content = tokio::fs::read_to_string(file_path).await
            .with_context(|| format!("Failed to read file: {}", file_path.display()))?;
            
        let mut entry: FileQueueEntry = serde_json::from_str(&content)
            .with_context(|| format!("Failed to parse JSON from: {}", file_path.display()))?;
            
        info!("üîß [TIER-3-FILE] Processing queue entry: {} (correlation: {})", 
              entry.id, entry.correlation_id);
        
        // Update status to processing
        entry.status = FileQueueStatus::Processing;
        entry.processed_at = Some(Utc::now());
        
        // Simulate guaranteed processing (1-5 seconds as per task requirements)
        let processing_delay = Duration::from_millis(1500);
        sleep(processing_delay).await;
        
        let processing_result = ProcessingResult {
            success: true,
            action: self.extract_action_from_payload(&entry.payload),
            task_id: entry.correlation_id.clone(),
            processing_time_ms: start_time.elapsed().as_millis() as u64,
            acknowledgment_sent: true,
            tier: "file_watcher".to_string(),
            correlation_id: entry.correlation_id.clone(),
            error: None,
        };
        
        // Update entry with completion status
        entry.status = FileQueueStatus::Completed;
        entry.processed_at = Some(Utc::now());
        
        // Move to processed directory
        self.move_to_processed(&entry, file_path).await?;
        
        // Update metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.total_processed += 1;
            metrics.successful_processed += 1;
            metrics.queue_depth = metrics.queue_depth.saturating_sub(1);
            
            // Update average processing time
            let new_time = processing_result.processing_time_ms;
            if metrics.total_processed == 1 {
                metrics.average_processing_time_ms = new_time;
            } else {
                let current_avg = metrics.average_processing_time_ms;
                metrics.average_processing_time_ms = 
                    ((current_avg * (metrics.total_processed - 1)) + new_time) / metrics.total_processed;
            }
        }
        
        info!("‚úÖ [TIER-3-FILE] Completed processing entry {} in {}ms", 
              entry.id, processing_result.processing_time_ms);
        
        Ok(processing_result)
    }
    
    fn extract_action_from_payload(&self, payload: &ResponsePayload) -> String {
        payload.callback_data
            .strip_prefix("action:")
            .unwrap_or("acknowledge")
            .to_string()
    }
    
    async fn move_to_processed(&self, entry: &FileQueueEntry, original_path: &Path) -> Result<()> {
        let filename = format!("{}.{}", entry.id, Self::QUEUE_FILE_EXTENSION);
        let processed_path = self.processed_directory.join(&filename);
        
        // Write updated entry to processed directory
        self.write_entry_to_directory(entry, &processed_path).await?;
        
        // Remove original file
        if let Err(e) = tokio::fs::remove_file(original_path).await {
            warn!("Failed to remove original file {}: {}", original_path.display(), e);
        }
        
        debug!("üìÅ [TIER-3-FILE] Moved to processed: {}", filename);
        Ok(())
    }
    
    async fn write_entry_to_directory(&self, entry: &FileQueueEntry, target_path: &Path) -> Result<()> {
        let json_data = serde_json::to_string_pretty(entry)
            .with_context(|| "Failed to serialize entry")?;
            
        tokio::fs::write(target_path, json_data).await
            .with_context(|| format!("Failed to write to {}", target_path.display()))?;
            
        Ok(())
    }
    
    pub fn recover_pending_entries(&self) -> Result<()> {
        let queue_files = fs::read_dir(&self.queue_directory)
            .with_context(|| format!("Failed to read queue directory: {}", self.queue_directory.display()))?;
            
        let mut recovered_count = 0;
        
        for entry in queue_files {
            let entry = entry?;
            let path = entry.path();
            
            if path.extension().map_or(false, |ext| ext == Self::QUEUE_FILE_EXTENSION) {
                // Check if file is valid JSON
                match fs::read_to_string(&path) {
                    Ok(content) => {
                        match serde_json::from_str::<FileQueueEntry>(&content) {
                            Ok(_) => {
                                recovered_count += 1;
                                debug!("üîÑ [TIER-3-FILE] Recovered queue entry: {}", 
                                       path.file_name().unwrap_or_default().to_string_lossy());
                            }
                            Err(e) => {
                                warn!("Invalid queue file {}: {}", path.display(), e);
                                // Move invalid files to failed directory
                                if let Some(filename) = path.file_name() {
                                    let failed_path = self.failed_directory.join(filename);
                                    if let Err(mv_err) = fs::rename(&path, &failed_path) {
                                        error!("Failed to move invalid file to failed directory: {}", mv_err);
                                    }
                                }
                            }
                        }
                    }
                    Err(e) => {
                        error!("Failed to read queue file {}: {}", path.display(), e);
                    }
                }
            }
        }
        
        if recovered_count > 0 {
            info!("üîÑ [TIER-3-FILE] Recovery completed: {} entries recovered", recovered_count);
            
            // Update metrics
            tokio::spawn(async move {
                // Note: This would need a reference to self.metrics, which is complex in this context
                // For now, we'll just log the recovery
            });
        }
        
        Ok(())
    }
    
    pub async fn get_metrics(&self) -> FileWatcherMetrics {
        self.metrics.read().await.clone()
    }
    
    pub async fn get_queue_status(&self) -> Result<(usize, usize, usize)> {
        let queue_count = self.count_files_in_directory(&self.queue_directory).await?;
        let processed_count = self.count_files_in_directory(&self.processed_directory).await?;
        let failed_count = self.count_files_in_directory(&self.failed_directory).await?;
        
        Ok((queue_count, processed_count, failed_count))
    }
    
    async fn count_files_in_directory(&self, directory: &Path) -> Result<usize> {
        let mut entries = tokio::fs::read_dir(directory).await
            .with_context(|| format!("Failed to read directory: {}", directory.display()))?;
            
        let mut count = 0;
        while let Some(entry) = entries.next_entry().await? {
            let path = entry.path();
            if path.extension().map_or(false, |ext| ext == Self::QUEUE_FILE_EXTENSION) {
                count += 1;
            }
        }
        
        Ok(count)
    }
    
    pub async fn cleanup_old_entries(&self, max_age_days: u32) -> Result<usize> {
        let cutoff_date = Utc::now() - chrono::Duration::days(max_age_days as i64);
        let mut cleaned_count = 0;
        
        for directory in [&self.processed_directory, &self.failed_directory] {
            let mut entries = tokio::fs::read_dir(directory).await?;
            
            while let Some(entry) = entries.next_entry().await? {
                let path = entry.path();
                
                if path.extension().map_or(false, |ext| ext == Self::QUEUE_FILE_EXTENSION) {
                    // Read entry to check date
                    if let Ok(content) = tokio::fs::read_to_string(&path).await {
                        if let Ok(queue_entry) = serde_json::from_str::<FileQueueEntry>(&content) {
                            if queue_entry.created_at < cutoff_date {
                                if let Err(e) = tokio::fs::remove_file(&path).await {
                                    warn!("Failed to remove old entry {}: {}", path.display(), e);
                                } else {
                                    cleaned_count += 1;
                                    debug!("üßπ [TIER-3-FILE] Cleaned old entry: {}", 
                                           path.file_name().unwrap_or_default().to_string_lossy());
                                }
                            }
                        }
                    }
                }
            }
        }
        
        if cleaned_count > 0 {
            info!("üßπ [TIER-3-FILE] Cleanup completed: {} old entries removed", cleaned_count);
        }
        
        Ok(cleaned_count)
    }
}

// Background task runner for the file tier processor
impl FileTierProcessor {
    pub async fn run_background_processor(&self) -> Result<()> {
        info!("üöÄ [TIER-3-FILE] Starting background processor...");
        
        let mut interval = tokio::time::interval(Duration::from_millis(Self::DEBOUNCE_WINDOW_MS));
        
        loop {
            interval.tick().await;
            
            // Process any debounced events
            if let Err(e) = self.process_debounced_events().await {
                error!("Error processing debounced events: {}", e);
            }
            
            // Perform periodic cleanup (once per hour)
            if rand::random::<u8>() % 240 == 0 { // Approximately once per hour at 500ms intervals
                if let Err(e) = self.cleanup_old_entries(7).await { // Keep 7 days of history
                    error!("Error during periodic cleanup: {}", e);
                }
            }
        }
    }
}